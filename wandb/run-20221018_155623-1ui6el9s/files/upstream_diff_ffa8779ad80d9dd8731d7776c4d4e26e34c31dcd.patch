diff --git a/.gitignore b/.gitignore
index e8c5c40..80f9e3e 100644
--- a/.gitignore
+++ b/.gitignore
@@ -17,3 +17,4 @@ particle_transformer/
 saved_plots/
 wandb/ 
 train_tensorflow/
+saved_plots copy/
\ No newline at end of file
diff --git a/git b/git
new file mode 100644
index 0000000..e69de29
diff --git a/lightning-hydra-template b/lightning-hydra-template
--- a/lightning-hydra-template
+++ b/lightning-hydra-template
@@ -1 +1 @@
-Subproject commit cd640a0153c117934ad95f55da2e1aa2c936d00f
+Subproject commit cd640a0153c117934ad95f55da2e1aa2c936d00f-dirty
diff --git a/models/model.py b/models/model.py
index 505d814..85e85a0 100644
--- a/models/model.py
+++ b/models/model.py
@@ -80,7 +80,7 @@ class LightningModel(pl.LightningModule):
                 yanchor="top",
                 y=0.99,
                 xanchor="left",
-                x=0.01)
+                x=0.01),
             margin=go.layout.Margin(
                 l=0,
                 r=0,
@@ -91,6 +91,8 @@ class LightningModel(pl.LightningModule):
                                                 color='red')))
         logger = self.trainer.logger.experiment
         logger.log({dist_name : fig})
+        logger.log({dist_name : kl_div_reweighted})
+        logger.log({dist_name + ' nominal' : kl_div})
     
     def compute_kls(self, dataset, dist_names, weights):
         """Computes KL divergence on a list of histograms
@@ -118,7 +120,7 @@ class LightningModel(pl.LightningModule):
             else:
                 bin_range = (0, 1)
             
-            nominal = compute_histogram(torch.tensor(x), bin_range=bin_range, density=True, weights=weights)
+            nominal = compute_histogram(torch.tensor(x), bin_range=bin_range, density=True)
             reweighted = compute_histogram(torch.tensor(x), bin_range=bin_range, density=True, weights=weights)
             target = compute_histogram(torch.tensor(y), bin_range=bin_range, density=True)
             
diff --git a/train.py b/train.py
index 6e7481a..3a7f5b9 100644
--- a/train.py
+++ b/train.py
@@ -80,13 +80,14 @@ trainer = Trainer(
     callbacks=[checkpoint_callback, progress_bar],
     accelerator='gpu',
     devices=1,
-    max_epochs=args.n_epochs,
+    max_epochs=2, #args.n_epochs,
     log_every_n_steps=1000,
     reload_dataloaders_every_n_epochs=args.reload_dataloader_every_n_epochs,
     check_val_every_n_epoch=20,
     fast_dev_run=False,
     num_sanity_val_steps=-1, # run a full validation epoch before starting training
-    logger=logger
+    logger=logger,
+    profiler='simple'
     
     # max_steps = 100000,
     # default_root_dir=args.root_dir,
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index bd740fe..b553529 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20221017_194436-1c9w4xpw/logs/debug-internal.log
\ No newline at end of file
+run-20221018_155623-1ui6el9s/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 97341fe..5403d7c 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20221017_194436-1c9w4xpw/logs/debug.log
\ No newline at end of file
+run-20221018_155623-1ui6el9s/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index cb98472..51e2481 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20221017_194436-1c9w4xpw
\ No newline at end of file
+run-20221018_155623-1ui6el9s
\ No newline at end of file
